---
title: LLMs for Preparing Data in R
author: Cynthia A. Huang
date: today
bibliography: references.bib
## toc: true
format:
  presentation-revealjs+letterbox:
    slide-number: true
---

```{r}
#| label: load-packages
library(knitr)
library(ggplot2)
library(dplyr)
```

# Introduction
<!-- 
## Today's lecture

::: callout-note
## What we'll cover
* The basics inputs and outputs of LLMs. 
* What data cleaning tasks LLMs can assist with. 
* How to appropriately cite and document data and code generated by LLMs. 
:::

. . .

::: callout-note
## Coding Perspective:

* Learn how to generate R code via LLM chat interfaces
* See how to interact with LLMs from within R using the ellmer package
::: -->

## Lecture outline

- About Me!
- About LLMs
- Using LLMs for "Wild Caught Data" tasks
- Using LLMs for WCD tasks in R with `{ellmer}`

## Learning goals

- develop your understanding of:
    - what LLMs are,
    - different types of "wild caught data" tasks that LLMs can help with,
    - how to use and check LLMs for specific data prepration tasks,
    - how to interact with LLMs in R using {ellmer}

# About Me!

## Who am I?

::: fragment
- üë©‚Äçüéì Research Fellow in EBS supervised by [_Prof. Rob Hyndman_](https://robjhyndman.com)
- üéôÔ∏è Regular host on [The Random Sample](https://www.therandomsample.com.au/podcasts/) podcast
- üë©üèª‚Äçüè´ Ex-Tutor for Wild Caught Data (2020)
:::
::: fragment
- üí± Previously:
  - Economics at the University of Melbourne
  - Catching wild data for empirical researchers:
      - wikipedia entries, archival magazines, trade databases, satellite images, online retail prices...
::: 

## What do I work on?

::: incremental
- üë©‚Äçüéì Thesis: **Unified Statistical Principles and Computational Tools for Data Harmonisation and Provenance**, supervised by:
  - [_Prof. Rob Hyndman_ (EBS, MBUS)](https://robjhyndman.com), [_Prof. Simon Angus_ (Econ, MBUS)](https://research.monash.edu/en/persons/simon-angus), and [_Dr. Sarah Goodwin_ (HCC, FIT)](https://research.monash.edu/en/persons/sarah-goodwin) 
:::
::: incremental
- üìä Research Interests
  - üå∞ Designing tools and workflows for wild caught data!
  - ü§ñ Leveraging LLMs and genAI for data wrangling and cleaning
    - üñáÔ∏è using LLMs to correct manual data entry errors -- Current MBAT research internship!
:::

# About Large Language Models

## Generative AI and LLMs

::: {.columns}
::: {.column}
According to Claude 3.7 Sonnet, *Generative AI* refers to:

- computer algorithms and systems
- that can generate content such as text, images and sound
- based on patterns learnt from existing data

<!-- Today, we will focus on *text* generation using Large Language Models (LLMs). -->
:::
::: {.column .fragment}
![](images/illustrations/wcd-guest-lecture/genAI-llm-tree.png)
:::
:::

## What are Large Language Models?

::: {.columns}
::: {.column}
LLMs are...

::: incremental
- code writers?
- encyclopedias?
- assignment help?
- translators?
:::
::: {.fragment}
We often understand tools by what they can do for us, not how they work.
:::
:::
::: {.column .fragment}
![](images/illustrations/wcd-guest-lecture/dishwasher-01-closed.png){fig-align="center" height=500px}
<!--- what does a dishwasher DO? vs. How does a dishwasher WORK? --->
:::
:::

<!-- LLMs are...

::: incremental
- code writers?
- encyclopedias?
- assignment help?
- translators? -->

## Who makes LLMs?

**LLM providers** develop and offer access to large language models and systems

![](assets/llm-providers-googlesearch.png)

[Today, we will see demos of _Anthropic_ and _OpenAI_ models.]{.fragment}

## Why are there so many different models?

::: {.columns}
::: {.column}
LLM providers offer paid and free access to multiple models:

- **OpenAI**: [GPT-3 and 4, o-series](https://platform.openai.com/docs/models/compare), 
- **Anthroptic**: [Claude Haiku, Sonnet and Opus](https://docs.anthropic.com/en/docs/about-claude/models/overview)
- **Google**: [Gemini Flash and Pro](https://cloud.google.com/vertex-ai/generative-ai/docs/models)
- **Meta**: [Llama 3, Llama 4](https://www.llama.com/docs/model-cards-and-prompt-formats/)
- **Alibaba**: [Gwen 2.5, 3, Max, Plus and Turbo](https://www.alibabacloud.com/en/solutions/generative-ai/qwen?_p_lc=1--------)

:::
::: {.column}

[Different models are designed to be good at different things:]{.fragment}

::: incremental

- chain-of-thought reasoning vs. instruction following
- multimodal support: images, audio, video AND text
- multilingual processng: translation, content generation
- specific domains: medicine, finance, legal

:::


<!-- Multiple models and capabilities can be used together

![](assets/openAI-cookbook.png) -->

:::
:::


## Model differentiation

Learn more:

- [üìñ OpenAI Model Selection Guide](https://cookbook.openai.com/examples/partners/model_selection_guide/model_selection_guide)
- [üéôÔ∏è Beyond ChatGPT: THE RAPIDLY EVOLVING LANDSCAPE OF AI](https://www.therandomsample.com.au/podcast/beyond-chatgpt-ai-landscape/)
- [üìñ Anthropic Claude Model Comparison Table](https://docs.anthropic.com/en/docs/about-claude/models/overview#model-comparison-table)


<!-- - domain-specific: biomedical ([bioBERT](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506)), finance, legal, scientific)
- task-specific (summarisation, question-answering, content creation)
- multimodal models (text and images - incl. video)
- multilingual -->

## How can we interact with LLMs?

<!--- TODO: re-export image -->

::: {.columns}
::: {.column width="35%"}
![](images/illustrations/wcd-guest-lecture/openAI-anthropic-models.png)
:::
::: {.column width="65%"}

::: {.fragment}
### 1. Web-based Chat Interface
- [ChatGPT](https://chatgpt.com), [Claude AI](https://claude.ai/), [Qwen Chat](https://chat.qwen.ai)
- offers additional formatting of outputs, access to tools, other 'quality of life' features
:::

::: {.fragment}
### 2. Programmatic Interfaces
- requires an API key
- interact using code with an LLM from within an R session
:::
<!-- ### 3. Other interfaces
- voice assistants
- embedded LLMs (e.g. text completion in Gmail) -->

:::
:::


## How can we interact with LLMs?

<!--- TODO: re-export image -->

::: {.columns}
::: {.column width="35%"}
![](images/illustrations/wcd-guest-lecture/openAI-anthropic-models.png)
:::
::: {.column width="65%"}
### 1. Web-based Chat Interface
### 2. Programmatic Access
### 3. Other interfaces
- mobile (chat) applications
- voice assistants
- embedded LLMs (e.g. suggestions in Gmail)

<!-- [Today, we will use web-based chat via [chatgpt.com](https://chatgpt.com) and [claude.ai](https://claude.ai/), as well as programmatic access via [`{ellmer}`](https://ellmer.tidyverse.org)]{.fragment} -->

:::
:::

## How can we interact with LLMs?

<!--- TODO: re-export image -->

::: {.columns}
::: {.column width="35%"}
![](images/illustrations/wcd-guest-lecture/openAI-anthropic-models.png)
:::
::: {.column width="65%"}
### 1. Web-based Chat Interface
### 2. Programmatic Access
### 3. Other interfaces

Today, we will use web-based chat via [chatgpt.com](https://chatgpt.com) and [claude.ai](https://claude.ai/), as well as programmatic access via [`{ellmer}`](https://ellmer.tidyverse.org)

:::
:::

<!-- [Today, we will try data preparation tasks using the both interfaces.]{.fragment} -->

## How do we verify what LLMs are doing?

::: {.columns}
::: {.column}

For a dishwasher, we consider:

::: {.incremental}
- **...are the dishes clean?**
- **is there any dirt on the dishes?**
:::

[For LLMs...?]{.fragment}

[It depends on the (data) task!]{.fragment}

:::
::: {.column .fragment}
![](images/illustrations/wcd-guest-lecture/dishwasher-02-opened.png){fig-align="centre" height=500px}
:::
:::

# Using LLMs for "Wild Caught Data" Tasks

## What are common WCD tasks?

::: {.columns}
::: {.column}
![](images/illustrations/wcd-guest-lecture/LLM-to-helpers_narrow.png){fig-align="centre" height=550px}
:::
::: {.column}

::: callout-note
## Discuss in groups [5 mins]

- What tasks are involved in preparing Wild Caught Data for analysis? _List at least 3._
- Which of these might be more or less suitable for addressing with LLMs?
- What would you think about when writing prompts (i.e. chatting with an LLM) for these tasks? 
:::

:::
:::



<!-- What tasks are involved in turning Wild Caught Data into analysis-ready data? -->

<!-- - obtaining data
- combining data
- **cleaning data**
    - missing data
    - 

How could we use LLMs for each task? -->

## Ways to use LLMs for Wild Caught Data tasks

1. code generation --> execute on your data
2. data generation --> directly modify or augment your data

- obtaining data:
- combining data:
- cleaning data:

<!-- ## Tool-User Beware!

insert mistake -->

## Live Demo (OpenAI - Chat GPT)

- famous person

## Live Demo (ChatGPT)

- numbats example: https://chatgpt.com/share/68283dc2-a164-8000-9587-40ebd232545e

## LLM Tool Usage ()

This works because of tool usage...
<!--- Let's turn off Tool Usage -->

## Live Demo (Claude 3.7 Sonnet with/without Web Search)

- point out interface differs (working version)
- now let's turn off (tool)


# LLMs as a DIRECT data cleaning tool

## TASK: what are some issues you've encountered with wild caught datasets???

## Wild Caught Dataset Issues

- factual look up (nationalities),
- missing data (volume) -- talk about prompts & context,
- inconsistencies (country codes, ST, st street),
- harmonisation/matching (occupation-- are these two the same?)),
- text summary/extraction (sentiment)

## Data 

Example 1:
Example 2:
Example 3:


## Verifying correctness

The most important skill to develop when using LLMs is verification skills. 
But 'how' to verify an LLM output is not always clear.
How do you check if an LLM output matches your needs?

<!--- Let's think about how to verify  --->

## Verifying data outputs

Assuming we don't know how LLMs work, just like we can't read other people's minds, how might we check if an LLM is doing the 'right thing'?

* Define characteristics of the 'right' output -- positive verification
* Figure out signals or signs of 'wrong things' -- negative verification

<!--- principle agent problem; management theory--> 

## 'External Knowledge' in LLMs

LLMs are trained on 'wild caught data' -- 
How do you think the data that LLMs are trained on affects their performance?

## How would you verify that the LLM is returning valid output reliably?

::: {.callout-note style="width: 75%; margin: 0 auto;"}
## Try it yourself time

- Try this thing 

- Also try this thing 

- Feeling adventurous do this

:::

## Reliable LLM Agents

- Agentic models are ...
- Tool calling and modularisation
- Structured output modes




# Beyond chatting with LLMs

<!---image: web interface vs. API -->

## Ways to interact with LLMs

- web-interface + copy/paste
- programmatically = code and variables

Using the {ellmer} R package we can:

- connect to an LLM
- send prompts to that LLM from an R session
- assign output from the LLM to variables

## Setting up {ellmer}

Requirements:

- API key -- knock! knock! who's there? Cynthia's R session!
- ellmer installed

## Live Demo

$livechat()
$chat()

## Why bother with programmatic interface?

- scale
- reproducibility
- build your own chatbot

Demo:

1. system prompts

## Live Demo (system prompts)

## What else? -- data extraction/cleaning

2. entity recognition -- $extract_data

## Constructing prompts from data with {ellmer}

Give data...


## Saving responses as data with {ellmer} / variables

Receive and save data

## Scaling up to an entire dataset!

- costs
- columm by column
- BREAK DOWN THE TASK!

## Evaluating performance?

<!--- introduce MBAT project --->

# Citing GenAI & LLM usage

## Monash Policy

## Acknowledging generative AI

I used AI in the following ways:
(i) generate definitions and suggested explanations for key concepts covered in this lecture.

(ii) generate text, rewrite, rephrase and/or paraphrase a portion of this assessment.
I used Microsoft Word with copilot assisting to prepare the essay drafts (4 iterations). I used GoodAI to help revise the introduction (3 iterations) and then ChatGPT3.5 (2 iterations) to make the introduction sound more academic. I further edited the introduction adding appropriate citations.
(iii) generate some other aspect of the submitted assessment. 
I tried Microsoft Excel with copilot assisting to make the graphs but found GraphMaker (https://www.graphmaker.ai/) to produce better results for creating the graphs used as evidence in the essay. I used ChatGPT3.5 to change the bibliographic references into notes form for use in the footnotes.

## Ethical concerns

- copyright
- celebritry likeness
- deepfakes
- environmental
- find some good articles on this

## Future of LLMs

- MCP -- supercharge tool development [read more](https://www.descope.com/learn/post/mcp)
- programming in natural language?
- broader LLM landscape -- differentiation and tools [xxx](https://cobusgreyling.medium.com/the-language-model-landscape-is-being-disrupted-again-e6e992c57f8e)
- translation
- evaluation** -- safety & bias

## Key takeaways

There are many LLM models and systems which:

- generate text outputs: code and 'data'
- call tools to get more (text) input
- have different types of user interfaces: chat vs. programmatic

When using LLMs for preparing data, think about:

- Breaking your larger, overall data preparation goal into *specific* tasks?
- Positive and negative verification of the LLM's performance on your task! 

*specific* tasks you're interested in achieving?



<!--- Model context protocol -->


## Summary

:::callout-note
## What we've learnt 

:::incremental 
- Learnt the basics of what LLMs can receive as inputs and generate as outputs
- Explored different 'wild caught data' tasks that LLMs can be used for
- Generated LLM outputs for these tasks including code and text data
- Learnt how the package {ellmer} can be used to interact with LLMs from R
- Discussed pitfalls to avoid when using LLMs for data cleaning
:::

:::


# ARCHIVE

## Training vs. Prompting

<!--- image: data in, questions in and answers out -->

Similar to you learning content during semester, 

## Directly generating data with LLMs

When might we want to generate data?
- mock data for ggtilecal

<!-- useful in making R packages -- also in Advanced R Programming (debugging assignment) -->

## Citing and documenting generated data

<!--- blog post --->

## Broader landscape: models, interfaces & applications

![](assets/language-model-landscape-v8_cobus-greyling.webp)

<https://cobusgreyling.medium.com/the-language-model-landscape-is-being-disrupted-again-e6e992c57f8e>


## Differentiation in Models

- domain-specific: biomedical ([bioBERT](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506)), finance, legal, scientific)
- task-specific (summarisation, question-answering, content creation)
- multimodal models (text and images - incl. video)
- multilingual

- [üîó Beyond ChatGPT: THE RAPIDLY EVOLVING LANDSCAPE OF AI](https://www.therandomsample.com.au/podcast/beyond-chatgpt-ai-landscape/)