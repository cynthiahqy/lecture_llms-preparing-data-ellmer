---
title: LLMs for Preparing Data in R
author: Cynthia A. Huang
date: today
toc: true
format:
  presentation-revealjs+letterbox:
    slide-number: true
---

```{r}
#| label: load-packages
library(knitr)
library(ggplot2)
library(dplyr)
```

# Introduction

## Today's lecture

::: callout-note
## What we'll cover
* The basics inputs and outputs of LLMs. 
* What data cleaning tasks LLMs can assist with. 
* How to appropriately cite and document data and code generated by LLMs. 
:::

. . .

::: callout-note
## Coding Perspective:

* Learn how to generate R code via LLM chat interfaces
* See how to interact with LLMs from within R using the ellmer package
:::

<!-- ## Learning goals

- develop your understanding of:
    - how LLMs work,
    - strengths and limitations of LLMs for different (data) tasks
    - how to evaluate the performance on an LLM
    - how to interact with LLMs in R -->

<!-- ## Lecture structure

- About Me
- Some relevant background on LLMs
- Thinking about how to use LLMs for "wild caught data" tasks
- {ellmer} demo -->

## About Me!

::: fragment
- üë©‚Äçüéì Research Fellow supervised by:
  - [_Prof. Rob Hyndman_](https://robjhyndman.com), NUMBATs, Econometrics & Business Statistics
:::
::: fragment
- üë©‚Äçüéì Thesis: **Unified Statistical Principles and Computational Tools for Data Harmonisation and Provenance**, with co-supervisors:
  - [_A.Prof Simon Angus_](https://research.monash.edu/en/persons/simon-angus), SoDa Laboratories, Economics
  - [_Dr. Sarah Goodwin_](https://research.monash.edu/en/persons/sarah-goodwin), EmVis group, Human Centered Computing
::: 

## About Me!

::: incremental
- üí± Previously: 
  - Economics at the University of Melbourne
  - Data collection & curation for empirical economists
:::
::: incremental
- üìä Research Interests
  - üå∞ 
  - ü§ñ Leveraging LLMs and genAI in data wrangling and preparation
  - üñáÔ∏è Currently supervising a MBAT research internship on using LLMs to correct manual data entry errors.
:::

# Under the hood of LLMs

## What are LLMs anyway?

LLMs are...

- lossy jpegs
- code writers?
- encyclopedias?
- assignment help?
- multi-purpose tools!

How your washing machine was built? vs. What your washing machine can do?

## Types of Models

- general knowledge vs. domain-specific (medical, )
- general purpose vs. task-specific (reasoning, conversation)

[üîó Beyond ChatGPT: THE RAPIDLY EVOLVING LANDSCAPE OF AI](https://www.therandomsample.com.au/podcast/beyond-chatgpt-ai-landscape/)

## Training vs. Prompting

<!--- image: data in, questions in and answers out -->

Similar to you learning content during semester, 

## Directly generating data with LLMs

When might we want to generate data?
- mock data for ggtilecal



<!-- useful in making R packages -- also in Advanced R Programming (debugging assignment) -->

## Citing and documenting generated data

<!--- blog post --->

## Breakout Session

::: {.callout-note style="width: 75%; margin: 0 auto;"}
## Try it yourself time

- Try this thing 

- Also try this thing 

- Feeling adventurous do this

:::

# What data preparation tasks might we want to use LLMs for?

## Wild Caught Data tasks

What tasks are involved in turning Wild Caught Data into analysis-ready data?

- obtaining data
- combining data
- **cleaning data**
    - missing data
    - 

Which of these might be more or less suitable for addressing with LLMs? How could we use LLMs for each task?

## Ways to use LLMs for Wild Caught Data tasks

1. code generation --> execute on your data
2. data generation --> directly modify or augment your data

- obtaining data:
- combining data:
- cleaning data:

<!-- ## Tool-User Beware!

insert mistake -->

# LLMs as a DIRECT data cleaning tool

## Data 

Example 1:
Example 2:
Example 3:

## Verifying correctness

The most important skill to develop when using LLMs is verification skills. 
But 'how' to verify an LLM output is not always clear.
How do you check if an LLM output matches your needs?

<!--- Let's think about how to verify  --->

## Verifying data outputs

Assuming we don't know how LLMs work, just like we can't read other people's minds, how might we check if an LLM is doing the 'right thing'?

* Define characteristics of the 'right' output -- positive verification
* Figure out signals or signs of 'wrong things' -- negative verification

<!--- principle agent problem; management theory--> 

## 'External Knowledge' in LLMs

LLMs are trained on 'wild caught data' -- 
How do you think the data that LLMs are trained on affects their performance?

## Breakout Session

::: {.callout-note style="width: 75%; margin: 0 auto;"}
## Try it yourself time

- Try this thing 

- Also try this thing 

- Feeling adventurous do this

:::


# Beyond chatting with LLMs

<!---image: web interface vs. API -->

## Ways to interact with LLMs

- web-interface + copy/paste
- programmatically = code and variables

Using the {ellmer} R package we can:

- connect to an LLM
- send prompts to that LLM from an R session
- assign output from the LLM to variables

## Setting up {ellmer}

Requirements:

- API key -- knock! knock! who's there? Cynthia's R session!
- ellmer installed

## Live Demo

$livechat()
$chat()

## Constructing prompts from data with {ellmer}

Give data...

## Saving responses as data with {ellmer}

Receive and save data

## Scaling up to an entire dataset!

- costs
- columm by column
- BREAK DOWN THE TASK!

## Evaluating performance?

<!--- introduce MBAT project --->



## Summary

:::callout-note
## What we've learnt 

:::incremental 
- Learnt the basics of what LLMs can receive as inputs and generate as outputs
- Explored different 'wild caught data' tasks that LLMs can be used for
- Generated LLM outputs for these tasks including code and text data
- Learnt how the package {ellmer} can be used to interact with LLMs from R
- Discussed pitfalls to avoid when using LLMs for data cleaning
:::

:::

