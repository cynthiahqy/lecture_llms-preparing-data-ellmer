---
title: LLMs for Preparing Data in R
subtitle: "Guest lecture for ETC5512: Wild Caught Data"
author: Cynthia A. Huang
date: today
bibliography: references.bib
code-copy: true
## toc: true
format:
  presentation-revealjs+letterbox:
    slide-number: true
filters:
  - include-code-files
---

```{r}
#| label: load-packages
library(knitr)
library(ggplot2)
library(dplyr)
```

# Introduction
<!-- 
## Today's lecture

::: callout-note
## What we'll cover
* The basics inputs and outputs of LLMs. 
* What data cleaning tasks LLMs can assist with. 
* How to appropriately cite and document data and code generated by LLMs. 
:::

. . .

::: callout-note
## Coding Perspective:

* Learn how to generate R code via LLM chat interfaces
* See how to interact with LLMs from within R using the ellmer package
::: -->

## Learning goals

- develop your understanding of:
    - what LLMs are,
    - different types of "wild caught data" tasks that LLMs can help with,
    - how to use and check LLMs for specific data prepration tasks,
    - how to interact with LLMs in R using {ellmer}

## Lecture outline

::: {.columns}
::: {.column width=35%}
![](images/illustrations/wcd-guest-lecture/openAI-anthropic-models.png){height=500px}
:::
::: {.column width=65%}
- About Me!
- About LLMs
- Using LLMs for "Wild Caught Data" tasks
- Using LLMs in R with `{ellmer}`
:::
:::



# About Me!

## Who am I?

::: fragment
- üë©‚Äçüéì Research Fellow in EBS supervised by [_Prof. Rob Hyndman_](https://robjhyndman.com)
- üéôÔ∏è Regular host on [The Random Sample](https://www.therandomsample.com.au/podcasts/) podcast
- üë©üèª‚Äçüè´ Ex-Tutor for Wild Caught Data (2020)
:::
::: fragment
- üí± Previously:
  - Economics at the University of Melbourne
  - Catching wild data for empirical researchers:
      - wikipedia entries, archival magazines, trade databases, satellite images, online retail prices...
::: 

## What do I work on?

::: incremental
- üë©‚Äçüéì Thesis: **Unified Statistical Principles and Computational Tools for Data Harmonisation and Provenance**, supervised by:
  - [_Prof. Rob Hyndman_ (EBS, MBUS)](https://robjhyndman.com), [_Prof. Simon Angus_ (Econ, MBUS)](https://research.monash.edu/en/persons/simon-angus), and [_Dr. Sarah Goodwin_ (HCC, FIT)](https://research.monash.edu/en/persons/sarah-goodwin) 
:::
::: incremental
- üìä Research Interests
  - üå∞ Designing tools and workflows for wild caught data!
  - ü§ñ Leveraging LLMs and genAI for data wrangling and cleaning
    - üñáÔ∏è using LLMs to correct manual data entry errors -- Current MBAT research internship!
:::

# About Large Language Models

## Generative AI and LLMs

::: {.columns}
::: {.column}
*Generative AI* refers to:

- computer algorithms and systems
- that can generate content such as text, images and sound
- based on patterns learnt from existing data

<!-- Today, we will focus on *text* generation using Large Language Models (LLMs). -->
:::
::: {.column .fragment}
![](images/illustrations/wcd-guest-lecture/genAI-llm-tree.png)
:::
:::

## What are Large Language Models?

::: {.columns}
::: {.column}
LLMs are...

::: incremental
- code writers?
- encyclopedias?
- assignment help?
- translators?
:::
::: {.fragment}
We often understand tools by what they can do for us, not how they work.
:::
:::
::: {.column .fragment}
![](images/illustrations/wcd-guest-lecture/dishwasher-01-closed.png){fig-align="center" height=500px}
<!--- what does a dishwasher DO? vs. How does a dishwasher WORK? --->
:::
:::

<!-- LLMs are...

::: incremental
- code writers?
- encyclopedias?
- assignment help?
- translators? -->

## Who makes LLMs?

**LLM providers** develop and offer access to large language models and systems

![](assets/llm-providers-googlesearch.png)

[Today, we will see demos of _Anthropic_ and _OpenAI_ models.]{.fragment}

## Why are there so many different models?

::: {.columns}
::: {.column}
LLM providers offer paid and free access to multiple models:

- **OpenAI**: [GPT-3 and 4, o-series](https://platform.openai.com/docs/models/compare), 
- **Anthroptic**: [Claude Haiku, Sonnet and Opus](https://docs.anthropic.com/en/docs/about-claude/models/overview)
- **Google**: [Gemini Flash and Pro](https://cloud.google.com/vertex-ai/generative-ai/docs/models)
- **Meta**: [Llama 3, Llama 4](https://www.llama.com/docs/model-cards-and-prompt-formats/)
- **Alibaba**: [Gwen 2.5, 3, Max, Plus and Turbo](https://www.alibabacloud.com/en/solutions/generative-ai/qwen?_p_lc=1--------)

:::
::: {.column}

[Different models are designed to be good at different things:]{.fragment}

::: incremental

- chain-of-thought reasoning vs. instruction following
- multimodal support: images, audio, video AND text
- multilingual processng: translation, content generation
- specific domains: medicine, finance, legal

:::


<!-- Multiple models and capabilities can be used together

![](assets/openAI-cookbook.png) -->

:::
:::


## Model differentiation

Learn more about picking the right tool:

- [üéôÔ∏è Beyond ChatGPT: THE RAPIDLY EVOLVING LANDSCAPE OF AI](https://www.therandomsample.com.au/podcast/beyond-chatgpt-ai-landscape/)
- [üí° Suggestions on provider/model choice in 'ellmer' docs](https://ellmer.tidyverse.org/index.html#providermodel-choice)
- [üìñ OpenAI Model Selection Guide](https://cookbook.openai.com/examples/partners/model_selection_guide/model_selection_guide)
- [üìñ Anthropic Claude Model Comparison Table](https://docs.anthropic.com/en/docs/about-claude/models/overview#model-comparison-table)


<!-- - domain-specific: biomedical ([bioBERT](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506)), finance, legal, scientific)
- task-specific (summarisation, question-answering, content creation)
- multimodal models (text and images - incl. video)
- multilingual -->

## How can we interact with LLMs?

<!--- TODO: re-export image -->

::: {.columns}
::: {.column width="35%"}
![](images/illustrations/wcd-guest-lecture/openAI-anthropic-models.png)
:::
::: {.column width="65%"}

::: {.fragment}
### 1. Web-based Chat Interface
- [ChatGPT](https://chatgpt.com), [Claude AI](https://claude.ai/), [Qwen Chat](https://chat.qwen.ai)
- offers additional formatting of outputs, access to tools, other 'quality of life' features
:::

::: {.fragment}
### 2. Programmatic Interfaces
- requires an API key
- interact using code with an LLM from within an R session
:::
<!-- ### 3. Other interfaces
- voice assistants
- embedded LLMs (e.g. text completion in Gmail) -->

:::
:::


## How can we interact with LLMs?

<!--- TODO: re-export image -->

::: {.columns}
::: {.column width="35%"}
![](images/illustrations/wcd-guest-lecture/openAI-anthropic-models.png)
:::
::: {.column width="65%"}
### 1. Web-based Chat Interface
### 2. Programmatic Access
### 3. Other interfaces
- mobile (chat) applications
- voice assistants
- embedded LLMs (e.g. suggestions in Gmail)

<!-- [Today, we will use web-based chat via [chatgpt.com](https://chatgpt.com) and [claude.ai](https://claude.ai/), as well as programmatic access via [`{ellmer}`](https://ellmer.tidyverse.org)]{.fragment} -->

:::
:::

## How can we interact with LLMs?

<!--- TODO: re-export image -->

::: {.columns}
::: {.column width="35%"}
![](images/illustrations/wcd-guest-lecture/openAI-anthropic-models.png)
:::
::: {.column width="65%"}
### 1. Web-based Chat Interface
### 2. Programmatic Access
### 3. Other interfaces

Today, we will use web-based chat via:

- [chatgpt.com](https://chatgpt.com)
- [claude.ai](https://claude.ai/), 

as well as programmatic access via: 

- [`{ellmer}`](https://ellmer.tidyverse.org)

:::
:::

<!-- [Today, we will try data preparation tasks using the both interfaces.]{.fragment} -->

## How do we verify what LLMs are doing?

::: {.columns}
::: {.column}

For a dishwasher, we consider:

::: {.incremental fragment-index=1}
- **...are the dishes clean?**
- **is there any dirt on the dishes?**
:::

[For LLMs...?]{.fragment}

[It depends on the (data) task!]{.fragment}

:::
::: {.column .fragment fragment-index=2}
![](images/illustrations/wcd-guest-lecture/dishwasher-02-opened.png){fig-align="centre" height=500px}
:::
:::

# Using LLMs for "Wild Caught Data" Tasks

## What are common WCD tasks?

::: {.columns}
::: {.column}
![](images/illustrations/wcd-guest-lecture/LLM-to-helpers_narrow.png){fig-align="centre" height=550px}
:::
::: {.column}

::: callout-note
## Discuss in groups [5 mins]

- What tasks are involved in preparing Wild Caught Data for analysis? _List at least 3._
- Which of these might be more or less suitable for addressing with LLMs? Why?
<!-- - What would you think about when writing prompts (i.e. chatting with an LLM) for these tasks?  -->
:::

:::
:::

<!-- What tasks are involved in turning Wild Caught Data into analysis-ready data? -->

<!-- - obtaining data
- combining data
- **cleaning data**
    - missing data
    - 

How could we use LLMs for each task? -->

## Using LLMs for Data Preparation Tasks

::: {.fragment}
### 1. Generating data wranging **code**
  - 'indirect' use of LLMs to clean data
:::

::: {.fragment}
### 2. Transformating and generating **new data**
  - creating example datasets (e.g. when documenting a custom R function)
  - changing data between formats (csv to json)
:::

## Using LLMs for Data Preparation Tasks

### 1. Generating data wranging **code**

### 2. Transformating and generating **new data**

::: {.fragment}
### 3. Modifying and augmenting **existing data**
  - filling in missing data
  - correct typos or inconsistencies
  - creating new columns based on existing ones
:::

## Modifying and augmenting data

::: {.columns}
::: {.column}

::: {.incremental}
- filling in missing data
  - **'look up' facts:** author nationality: `Jane Austen`
  - **suggesting values:** missing volume units for drinks: `300?`
- correct typos or inconsistencies
  - **harmonise different abbreviations:** `{Victoria, VIC, Vic}` --> `{VIC}`
:::

:::
::: {.column}

::: {.incremental}
- creating new columns based on existing ones
  - **comparison and categorisation:** are `teachers` and `instructors` similar occupations?
  - **summarise text:** key points in free-form survey responses
  - **extract info**: name of the movie in a film review
:::

:::
:::

## Prompts for Data Preparation Tasks

::: {.columns}
::: {.column .incremental}
- prompts need to include:
  - instruction and data!
- responses would ideally:
  - return data of the expected type
  - and in a easy to import format
:::
::: {.column .fragment}
::: {.callout-note}
## Try yourself! [10mins]

1. Pick a starting prompt from the next page
2. Fill in the necessary data.
3. Try the prompt in your choice of LLM chat (e.g. ChatGPT, Qwen Chat, Claude AI etc.). 
    - What output did you get in return?
    - Could you import it into R easily?
4. Modify the prompt to return the answer in a more useful format.

:::
:::
:::



## Starting Prompts

- **'look up' facts:** "What nationality is _the author_ `<author name>`?"
- **suggesting values:** "What is the likely volume unit _of a beverage `<can>` of with_ a volume of `<300>`?"
- **harmonise different abbreviations:** "Convert the following list of Australian states to all use three-letter state codes: `<list>`" <!-- -->
- **comparison:** "How similar are these two occupations: `<occupation A>`, `<occupation B>`?"
- **summary:** "Summarise the following survey response: `<text>`"
- **extraction**: "What movie is the follow review about": `<review text>`

## Requesting different output formats

::: incremental
- LLM can respond in many different 'text' formats.
- Some are more useful than others.
:::

::: {.fragment}
Let's look at an [example conversation with ChatGPT](https://chatgpt.com/share/6828bf20-3600-8000-b26d-830a9fde7728) for the following prompt

```
Convert the following list of Australian states to all use three-letter state codes (e.g. VIC, TAS):
- Victoria
- NSW
- N.T.
- ACT
- Queensland
```
:::


## Verifying success

::: {.columns}
::: {.column}

**Verification** is the most important skill when using LLMs, but it requires:

* Clearly defined tasks and expected outcomes
* Ways of checking the outcomes have been achieved

:::
::: {.column}
![](images/illustrations/wcd-guest-lecture/dishwasher-03-clean.png)
:::
:::

::: notes
How can we check if the LLM output is accurate or valid?

First, you need to understand and define the task.

Next, consider **How would you check if a human completed this task correctly?**

One of the most important skills to develop when using LLMs is **verification**. 

But 'how' to verify an LLM output is not always clear.
:::

<!--- principle agent problem; management theory--> 

## Approaches to verification

::: {.columns}
::: {.column}

There are multiple ways to verify outcomes match expectations.

::: {.incremental}
* **Positive verification:** Define characteristics of 'success'
* **Negative verification:** Figure out signals or signs of 'failure'
* **Trust-based verification:** Seek assurance and confirmation of 'success'
:::

:::
::: {.column .fragment}
![](images/illustrations/wcd-guest-lecture/dishwasher-04-dirty.png)
:::
:::

## Checking on the dishes

::: {.columns}
::: {.column}

There are multiple ways to verify outcomes match expectations.

* **Positive verification:** [Are the dishes clean?]{.fragment}
* **Negative verification:** [Is there any dirt on the dishes?]{.fragment}
* **Trust-based verification:** [Ask the machine if the dishes are clean...?]{.fragment}

:::
::: {.column}
![](images/illustrations/wcd-guest-lecture/dishwasher-04-dirty.png)
:::
:::

# Using LLMs in R with {ellmer}

<!---image: web interface vs. API -->

## Beyond web-based interfaces

Different interfaces mean different data preparation workflows:

::: {.incremental}
- LLM web-interface = copy/paste
- programmatic interfaces = code and variables
:::


Using the {ellmer} R package we can:

::: {.incremental}
- send prompts to that LLM from an R session
- construct prompts from with imported data
- systematically docutest different prompts BEFORE scaling up
- manipulate response content using code
:::

## Setting up {ellmer}

::: {.columns}
::: {.column}
Start by:

- Installing [`{ellmer}`](https://ellmer.tidyverse.org/)

- Getting an API key from the LLM provider you want to use
  
  > knock! knock! who's there? Cynthia's R session!

- Storing the API key where ellmer can find it

:::
::: {.column}
Then start chatting:

```r
library(ellmer)

session <- chat_openai()
```

[A **session** is a single conversation instance between a user and an LLM.]{.fragment}

:::
:::

## System Prompts

::: {.columns}
::: {.column}
A **system prompt** is the behind-the-scenes instruction manual that tells an AI assistant:

- what tone to use,
- what information the system can access,
- and how to handle different types of questions or requests
:::
::: {.column}

```{.r include="lecture_ellmer-demo.R"}
```
Example adapted from [ellmer docs](https://ellmer.tidyverse.org/articles/prompt-design.html)
:::
:::

## Revisiting author nationality

```{r}
author_df <- readr::read_csv('example_data/week10-author_df.csv')

missing_nat <- author_df |> 
  dplyr::mutate(nationality = na_if(nationality, "No nationality matched"))# |> 
  #dplyr::filter(is.na(nationality))

missing_nat

```

##  Revisiting author nationality

```r
chat <- chat_openai()
```

## Evaluation through agreement

Another way to verify is via **consensus**.

In the author nationality example, we could compare nationality responses from different models.

Would you trust the data more if you got all the same answers? or all different answers?


## LLM Agents

::: {layout-ncol="2"}

![](assets/claude-tool-settings.png)

![](assets/chatgpt-tool-use.png)

:::

<!--- try in Tutorial -->

<!-- ## Scaling up to an entire dataset!

- costs
- columm by column
- BREAK DOWN THE TASK! -->

# Final Comments

## Generative AI acknowledgement

::: {.callout-important}

##

I used AI in the following ways:

(i) generate definitions and suggested explanations for key concepts covered in this lecture. I used Claude AI to suggest definitions for terms like 'Generative AI', and 'System Prompt', and to generate lists of "top LLM providers in 2025" and "ways of interacting with LLMs".

:::

## Key takeaways

::: {.fragment}

There are many LLM models and systems which:

- generate text outputs: code and 'data'
- call tools to get more (text) input
- are available via different types of user interfaces: chat vs. programmatic

:::


::: {.fragment}
When using LLMs for preparing data, think about:

- Breaking your larger, overall data preparation goal into *specific* tasks
- Ways to verify the LLM's performance on your task
:::

## What we've learnt...

::: {.columns}
::: {.column}
- basics of what LLMs can do, who provides them and what differentiates them
- different 'wild caught data' tasks that LLMs can be used for
- how to approach verifying LLM output for data preparation tasks
- how to interact with LLMs from R using {ellmer}
:::
::: {.column}
![](images/illustrations/wcd-guest-lecture/LLM-to-helpers_narrow.png){fig-align="centre" height=550px}
:::
:::


